{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablo-sampaio/espacos4.0_pe/blob/main/IA_Python_03_AprendizagemReforco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Aprendizagem de Máquina - Parte 3\n",
        "\n",
        "Neste notebook, mostramos rapidamente como usar **aprendizagem por reforço** para resolver uma tarefa simples. \n",
        "\n",
        "> Vamos usar o algoritmo **PPO** (do módulo chamado *stable-baselines*) \n",
        "\n",
        "> O ambiente é chamado de **LunarLander-v2** e ele simula a tarefa de pousar um \"módulo lunar\" (oferecido pelo módylo *gym*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thRA4CT9kH4G"
      },
      "source": [
        "## 1 - Instalação de pacotes \n",
        "\n",
        "> Atenção: execute 1 vez esta seção e, depois, reinicie o ambiente (menu `Ambiente de Execução` $\\rightarrow$ `Reiniciar ambiente de execução`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install gym[all]==00.25.1\n",
        "!pip install pyglet\n",
        "!pip install stable-baselines3[extra]==1.6.0\n",
        "!pip install importlib-metadata==4.13\n",
        "\n",
        "#clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "## 2 - Definições e Importações\n",
        "\n",
        "> Depois de executar a seção 1 e reiniciar o ambiente, você pode executar daqui para baixo!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLlWktrRj9GZ"
      },
      "source": [
        "### 2.1 Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "outputs": [],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record the given number of steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "### 2.2 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.ppo import MlpPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## 3 - Cria o ambiente e instancia o agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0wyWYEKa5xo"
      },
      "source": [
        "Abaixo, criamos o ambiente da tarefa **LunarLander-v2** ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [],
      "source": [
        "# Diferença em relação ao vídeo: criei uma variável para guardar o nome do ambiente\n",
        "# Nomes de ambientes para testar: \"CartPole-v1\", \"BipedalWalker-v3\", \"MountainCar-v0\", \"LunarLander-v2\"\n",
        "# Veja mais em https://www.gymlibrary.dev/\n",
        "\n",
        "NOME_AMBIENTE = \"LunarLander-v2\"\n",
        "\n",
        "ambiente = gym.make(NOME_AMBIENTE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguir, criamos o modelo (ou agente) do tipo **PPO**, passando este dois argumentos principais:\n",
        "- o ambiente que o agente irá executar no seu aprendizado \n",
        "- *MlpPolicy* para fazer o agente usar uma rede neural MLP internamente."
      ],
      "metadata": {
        "id": "4_pWr2ZaRxKQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvZeI4RI88vQ"
      },
      "outputs": [],
      "source": [
        "modelo = PPO(MlpPolicy, ambiente, tensorboard_log=\"log_dir\", verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Vamos deixar o modelo/agente interagir com esse ambiente antes do treinamento, para comparar depois:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "outputs": [],
      "source": [
        "recompensa_media, _ = evaluate_policy(modelo, ambiente, n_eval_episodes=30, deterministic=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Recompensa média: {recompensa_media:.2f}\")"
      ],
      "metadata": {
        "id": "KLbMQ4IiSyjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTbAq8Ji_EBf"
      },
      "source": [
        "E agora vamos gravar um vídeo para vê-lo em ação, antes de treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKo8i2aw_Iz5"
      },
      "outputs": [],
      "source": [
        "record_video(NOME_AMBIENTE, modelo, video_length=1000, prefix='ppo-sem-treino')\n",
        "show_videos('videos', prefix='ppo-sem-treino')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "## 4 - Ativa o Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0ZVznfSt_AL"
      },
      "source": [
        "Aqui, vamos ativar um recurso para acompanhar o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MkjYOgx3itp"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## 5 - Treina o agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT-lCHMOAHR3"
      },
      "source": [
        "Abaixo, rodamos a função de treinamento. Ela irá executar a simulação várias vezes (por vários *episódios*) até completar 150 mil passos de simulação (150 mil ações executadas pelo agente)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [],
      "source": [
        "modelo.learn(total_timesteps=150000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF5htH8TAK8m"
      },
      "source": [
        "Depois, gravamos o vídeo do agente treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olb37dOk_rgz"
      },
      "outputs": [],
      "source": [
        "record_video(NOME_AMBIENTE, modelo, video_length=4000, prefix='ppo-treinado')\n",
        "show_videos('videos', prefix='ppo-treinado')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, vamos fazer o agente treinado executar ações no ambiente. Compare com o desempenho antes de treinar."
      ],
      "metadata": {
        "id": "CF9LnRPcTSI-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SoCtCVAiX0O"
      },
      "outputs": [],
      "source": [
        "recompensa_media, _ = evaluate_policy(modelo, ambiente, deterministic=False, n_eval_episodes=30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Recompensa média: {recompensa_media:.2f}\")"
      ],
      "metadata": {
        "id": "B-MFqq_8TrIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "thRA4CT9kH4G",
        "xVm9QPNVwKXN",
        "pLlWktrRj9GZ",
        "FtY8FhliLsGm"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}